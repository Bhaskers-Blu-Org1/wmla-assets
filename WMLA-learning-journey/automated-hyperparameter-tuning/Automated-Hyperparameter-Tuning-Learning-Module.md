

## Summary
TODO



## Description
TODO



## Instructions

The detailed steps for this tutorial can be found in the associated xx.  

Learn how to:

- Make changes to your code
- Make your dataset available
- Set up API end point and log on
- Submit job via API
- Monitor running job
- Retrieve output and saved models
- Debug any issues


## Changes to your code

Note that the code sections below show a comparison between the "before" and "HPO enabled" versions of the code using `diff`.

1. Get the WMLA cluster RESULT_DIR and LOG_DIR for the hpo training job. The RESULT_DIR can be used for final model saving, and the LOG_DIR can be used for user logs and monitoring.

&nbsp;
&nbsp;
![image1](https://raw.githubusercontent.com/IBM/wmla-assets/zhuangxy-patch-1/WMLA-learning-journey/automated-hyperparameter-tuning/shared_images/hpo_update_model_1.png)
&nbsp;
&nbsp;

2.  Replace the hyperparameter definition code by reading hyperparameters from the `config.json` file. the `config.json` is generated by WMLA HPO, whichcontains a set of hyperparameter candidates for each tuning jobs. The hyperparameters and the search space is defined when submitting the HPO task.

&nbsp;
&nbsp;
<!-- ![alt text](https://raw.githubusercontent.com/IBM/wmla-assets/master/WMLA-learning-journey/shared-images/2_model_update.png)
![alt text](https://raw.githubusercontent.com/IBM/wmla-assets/master/WMLA-learning-journey/shared-images/3_model_update.png) -->
![image2](https://raw.githubusercontent.com/IBM/wmla-assets/zhuangxy-patch-1/WMLA-learning-journey/automated-hyperparameter-tuning/shared_images/hpo_update_model_2.png)
