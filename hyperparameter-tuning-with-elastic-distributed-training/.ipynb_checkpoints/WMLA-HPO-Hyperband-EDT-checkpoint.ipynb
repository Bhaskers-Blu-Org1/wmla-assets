{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning with Elastic Distributed Training example notebook for WMLA\n",
    "\n",
    "This is a sample notebook showing how to use the \"Bring Your Own Framework\" API of WMLA to tune community PyTorch\n",
    "Vision model with HyperParameter Optimization and Elastic Distributed Training of Watson ML Accelerator. \n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "This example uses a Elastic Distributed instance group.\n",
    "\n",
    "[Kelvin Lui](https://w3.ibm.com/search/#/search?query=Kelvin%20lui)\n",
    "\n",
    "Based on examples from https://github.ibm.com/kelvinl/WMLA-enablement/ and specifically `WMLA EDT-Copy2`.\n",
    "\n",
    "BYOF creates execution tasks via API - this is the equivalent of running this command via the `dlicmd` command locally on the cluster. Details of the `dlicmd` command can be found in [dlicmd.py reference](https://www.ibm.com/support/knowledgecenter/en/SSFHA8_1.2.1/cm/dlicmd.html) in Knowledge Centre. \n",
    "\n",
    "Note: this setup assumes that models can access data sources local to the WMLA cluster - although this data could be downloaded during the model training phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "<a id='WMLA-HPO-Hyperband-Wells-EDT'></a>\n",
    "- [Log on](#Log-on)\n",
    "- [Health Check](#Health-Check)\n",
    "- [Update Model](#Update-model)\n",
    "- [Launch HPO Task](#Launch-HPO-Task)\n",
    "- [Check HPO Status](#Check-HPO-Status)\n",
    "- [Manage HPO tasks](#Manage-HPO-Task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Common Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For WMLA cluster, URLs required are:\n",
    "```\n",
    "$ egosh client view ASCD_REST_BASE_URL_1 | grep DESCRIPTION | cut -d' ' -f2\n",
    "http://hostname.local:8280/platform/rest/\n",
    "```\n",
    "Giving: http://hostname.local:8280/platform/rest/conductor/v1\n",
    "\n",
    "```\n",
    "$ egosh client view DLPD_REST_BASE_URL_1 | grep DESCRIPTION | cut -d' ' -f2\n",
    "http://hostname.local:9243/platform/rest/\n",
    "```\n",
    "Giving: http://hostname.local:9243/platform/rest/deeplearning/v1\n",
    "\n",
    "API documentation links (can also be found on both links above):\n",
    "\n",
    "- IBM Spectrum Conductor RESTful APIs: http://hostname.local:8280/cloud/apis/explorer/\n",
    "- IBM Spectrum Conductor Deep Learning Impact RESTful APIs: http://hostname.local:9243/cloud/apis/explorer/\n",
    "- Also further details on Knowledge Centre: https://www.ibm.com/support/knowledgecenter/en/SSWQ2D_1.2.1/cm/deeplearning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://colonia04.platform:8280/platform/rest/conductor/v1\n",
      "http://colonia04.platform:9280/platform/rest/deeplearning/v1\n"
     ]
    }
   ],
   "source": [
    "# Environment details:\n",
    "\n",
    "#master_host = '**** ADD HERE ****'\n",
    "#master_host = 'dse-ac922h.cpolab.ibm.com'\n",
    "#dli_rest_port = '9243'\n",
    "#sc_rest_port = '8643'\n",
    "\n",
    "master_host = 'colonia04.platform'\n",
    "#master_host = 'aienterprise4.aus.stglabs.ibm.com'\n",
    "dli_rest_port = '9280'\n",
    "sc_rest_port = '8280'\n",
    "protocol = 'http'\n",
    "\n",
    "sc_rest_url =  protocol+'://'+master_host+':'+sc_rest_port+'/platform/rest/conductor/v1'\n",
    "dl_rest_url = protocol+'://'+master_host+':'+dli_rest_port+'/platform/rest/deeplearning/v1'\n",
    "\n",
    "print (sc_rest_url)\n",
    "print (dl_rest_url)\n",
    "# User login details\n",
    "#wmla_user = '**** ADD HERE ****'\n",
    "#wmla_pwd = '**** ADD HERE ****'\n",
    "\n",
    "wmla_user = 'Admin'\n",
    "wmla_pwd = 'Admin'\n",
    "\n",
    "myauth = (wmla_user, wmla_pwd)\n",
    "\n",
    "# Spark instance group details\n",
    "#sig_name = '**** ADD HERE ****'\n",
    "#sigName = 'SIG-DSE-EDT'\n",
    "sigName = 'WellsFargoPoC'\n",
    "#sigName ='sig243-edt'\n",
    "\n",
    "# REST call variables\n",
    "commonHeaders = {'Accept': 'application/json'}\n",
    "\n",
    "\n",
    "#startTuneUrl='%s://%s:%s/platform/rest/deeplearning/v1/hypersearch' % (protocol, master_host, dli_rest_port)\n",
    "#sc_rest_url ='%s://%s:%d/platform/rest/conductor/v1' % (protocol, hostname, conductorport)\n",
    "\n",
    "req = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log On\n",
    "<a id='Log-on'></a>\n",
    "\n",
    "[back to top](#WMLA-HPO-Hyperband-Wells-EDT)\n",
    "\n",
    "Obtain login session tokens to be used for session authentication within the RESTful API. Tokens are valid for 8 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logon succeeded\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(sc_rest_url+'/auth/logon', verify=False, auth=myauth, headers=commonHeaders) \n",
    "\n",
    "if r.ok:\n",
    "    print ('\\nLogon succeeded')\n",
    "    \n",
    "else: \n",
    "    print('\\nLogon failed with code={}, {}'. format(r.status_code, r.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health Check\n",
    "[back to top](#WMLA-HPO-Hyperband-Wells-EDT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there is any existing hpo tasks and also verify the platform health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest API: **GET platform/rest/deeplearning/v1/hypersearch**\n",
    "- Description: Get all the hpo task that the login user can access.\n",
    "- OUTPUT: A list of hpo tasks and each one with the same format which can be found in the api doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getTuneStatusUrl: http://colonia04.platform:9280/platform/rest/deeplearning/v1/hypersearch\n",
      "Hpo task: Admin-hpo-865764150117434, State: FAILED\n",
      "Hpo task: Admin-hpo-1101261874485516, State: FAILED\n",
      "Hpo task: Admin-hpo-3651752950675102, State: FAILED\n",
      "Hpo task: Admin-hpo-3651791970264588, State: FAILED\n",
      "Hpo task: Admin-hpo-3652656031423039, State: FAILED\n",
      "Hpo task: Admin-hpo-3652848133085365, State: FAILED\n",
      "Hpo task: Admin-hpo-4402508485777365, State: RECOVERPEND\n",
      "Hpo task: Admin-hpo-4404453847231966, State: FINISHED\n",
      "Hpo task: Admin-hpo-4405121817174855, State: RECOVERPEND\n",
      "Hpo task: Admin-hpo-17975643951593, State: FAILED\n",
      "Hpo task: Admin-hpo-18176491357360, State: FAILED\n",
      "Hpo task: Admin-hpo-26662761022292, State: FAILED\n",
      "Hpo task: Admin-hpo-29763538447983, State: FAILED\n",
      "Hpo task: Admin-hpo-41642042184978, State: FAILED\n",
      "Hpo task: Admin-hpo-42385955978459, State: FINISHED\n",
      "Hpo task: Admin-hpo-45405500448823, State: FINISHED\n",
      "Hpo task: Admin-hpo-45723267117425, State: FINISHED\n",
      "Hpo task: Admin-hpo-46153013776230, State: FINISHED\n",
      "Hpo task: Admin-hpo-63180378205900, State: FINISHED\n",
      "Hpo task: Admin-hpo-67304712365680, State: FINISHED\n",
      "Hpo task: Admin-hpo-69265660731839, State: FINISHED\n",
      "Hpo task: Admin-hpo-81103168190973, State: FAILED\n",
      "Hpo task: Admin-hpo-83966261958354, State: FINISHED\n",
      "Hpo task: Admin-hpo-87038657810309, State: FAILED\n",
      "Hpo task: Admin-hpo-87482324743298, State: FAILED\n",
      "Hpo task: Admin-hpo-88012895662112, State: FINISHED\n",
      "Hpo task: Admin-hpo-389761848418821, State: FINISHED\n",
      "Hpo task: Admin-hpo-446861797606487, State: FINISHED\n",
      "Hpo task: Admin-hpo-470996705325006, State: FINISHED\n",
      "Hpo task: Admin-hpo-557106790244503, State: FAILED\n",
      "Hpo task: Admin-hpo-557386867420795, State: FINISHED\n",
      "Hpo task: Admin-hpo-559169605924197, State: FINISHED\n",
      "Hpo task: Admin-hpo-606424273611363, State: FINISHED\n",
      "Hpo task: Admin-hpo-606677097588552, State: FINISHED\n",
      "Hpo task: Admin-hpo-607368853527081, State: FINISHED\n",
      "Hpo task: Admin-hpo-608844830397827, State: FINISHED\n",
      "Hpo task: Admin-hpo-609194284938035, State: FINISHED\n",
      "Hpo task: Admin-hpo-609556557992080, State: FINISHED\n",
      "Hpo task: Admin-hpo-617791427561063, State: FINISHED\n",
      "Hpo task: Admin-hpo-618444097370237, State: FINISHED\n",
      "Hpo task: Admin-hpo-1161366121811487, State: FAILED\n",
      "Hpo task: Admin-hpo-1161798424168716, State: FAILED\n",
      "Hpo task: Admin-hpo-1167963322865448, State: FAILED\n",
      "Hpo task: Admin-hpo-1168189178261145, State: FINISHED\n",
      "Hpo task: Admin-hpo-1168517132139967, State: FINISHED\n",
      "Hpo task: Admin-hpo-1169021314895654, State: FINISHED\n",
      "Hpo task: Admin-hpo-1566215048207446, State: FINISHED\n",
      "Hpo task: Admin-hpo-1566338679263207, State: FINISHED\n",
      "Hpo task: Admin-hpo-1566523649870405, State: FINISHED\n",
      "Hpo task: Admin-hpo-1566781334871123, State: FINISHED\n",
      "Hpo task: Admin-hpo-1567145515152659, State: FINISHED\n",
      "Hpo task: Admin-hpo-1567334554890746, State: FINISHED\n",
      "Hpo task: Admin-hpo-1568532179824168, State: FINISHED\n",
      "Hpo task: Admin-hpo-1569330129356702, State: FINISHED\n",
      "Hpo task: Admin-hpo-1898512439712804, State: FAILED\n",
      "Hpo task: Admin-hpo-1900263284000323, State: FAILED\n",
      "Hpo task: Admin-hpo-1900335486254509, State: FAILED\n",
      "Hpo task: Admin-hpo-1900656823910456, State: FINISHED\n",
      "Hpo task: Admin-hpo-1901456134303343, State: FINISHED\n",
      "Hpo task: Admin-hpo-1903573948892416, State: FINISHED\n",
      "Hpo task: Admin-hpo-2146582700656527, State: FINISHED\n",
      "Hpo task: Admin-hpo-2146591994601826, State: FINISHED\n",
      "Hpo task: Admin-hpo-2146649597597590, State: FINISHED\n",
      "Hpo task: Admin-hpo-2153130229843508, State: FAILED\n",
      "Hpo task: Admin-hpo-2153558909501098, State: FAILED\n",
      "Hpo task: Admin-hpo-2153687660264341, State: FAILED\n",
      "Hpo task: Admin-hpo-2153961023309354, State: FAILED\n",
      "Hpo task: Admin-hpo-2154886461231315, State: FINISHED\n",
      "Hpo task: Admin-hpo-2156650551587127, State: FINISHED\n",
      "Hpo task: Admin-hpo-2158842191013843, State: FAILED\n",
      "Hpo task: Admin-hpo-2159662713931510, State: FAILED\n",
      "Hpo task: Admin-hpo-2159835421481356, State: FINISHED\n",
      "Hpo task: Admin-hpo-2160984232559120, State: FINISHED\n",
      "Hpo task: Admin-hpo-2163398104247968, State: FINISHED\n",
      "Hpo task: Admin-hpo-2234900686966786, State: FINISHED\n",
      "Hpo task: Admin-hpo-2239018333806010, State: FINISHED\n",
      "Hpo task: Admin-hpo-2242935737224386, State: FINISHED\n",
      "Hpo task: Admin-hpo-2307649793377906, State: FINISHED\n",
      "Hpo task: Admin-hpo-2307674584046913, State: FINISHED\n",
      "Hpo task: Admin-hpo-2790563413573252, State: FINISHED\n",
      "Hpo task: Admin-hpo-2791372015805076, State: FAILED\n",
      "Hpo task: Admin-hpo-2791559858612346, State: FINISHED\n",
      "Hpo task: Admin-hpo-2805125680235872, State: FINISHED\n",
      "Hpo task: Admin-hpo-40894637677181, State: FINISHED\n",
      "Hpo task: Admin-hpo-41486071064170, State: FINISHED\n",
      "Hpo task: Admin-hpo-42306620676909, State: FINISHED\n",
      "Hpo task: Admin-hpo-43989551935312, State: FAILED\n",
      "Hpo task: Admin-hpo-64737517805674, State: FAILED\n",
      "Hpo task: Admin-hpo-121935856803278, State: FINISHED\n",
      "Hpo task: Admin-hpo-738373071509316, State: FAILED\n",
      "Hpo task: Admin-hpo-738739774847379, State: FAILED\n",
      "Hpo task: Admin-hpo-738930529288227, State: FAILED\n",
      "Hpo task: Admin-hpo-739278498851266, State: FAILED\n",
      "Hpo task: Admin-hpo-739423876295559, State: FAILED\n",
      "Hpo task: Admin-hpo-739906264563547, State: FAILED\n",
      "Hpo task: Admin-hpo-740054011079503, State: FAILED\n",
      "Hpo task: Admin-hpo-740363184609365, State: FAILED\n",
      "Hpo task: Admin-hpo-740781608704470, State: FAILED\n",
      "Hpo task: Admin-hpo-741133495482657, State: FAILED\n",
      "Hpo task: Admin-hpo-741412551610451, State: FINISHED\n",
      "Hpo task: Admin-hpo-741903729361931, State: FAILED\n",
      "Hpo task: Admin-hpo-742227038621024, State: FAILED\n",
      "Hpo task: Admin-hpo-742299703652403, State: FINISHED\n",
      "Hpo task: Admin-hpo-742368331242798, State: FINISHED\n",
      "Hpo task: Admin-hpo-754810816615675, State: FINISHED\n",
      "Hpo task: Admin-hpo-755496000997484, State: FAILED\n",
      "Hpo task: Admin-hpo-755574084707714, State: FAILED\n",
      "Hpo task: Admin-hpo-755680135155285, State: FAILED\n",
      "Hpo task: Admin-hpo-758371806102875, State: FINISHED\n",
      "Hpo task: Admin-hpo-821939808891847, State: FINISHED\n",
      "Hpo task: Admin-hpo-116316534499867, State: FAILED\n",
      "Hpo task: Admin-hpo-116521512935854, State: FAILED\n",
      "Hpo task: Admin-hpo-117404793807015, State: FAILED\n",
      "Hpo task: Admin-hpo-124800747212736, State: FINISHED\n",
      "Hpo task: Admin-hpo-124954893644731, State: RECOVERPEND\n",
      "Hpo task: Admin-hpo-128724212896952, State: FINISHED\n"
     ]
    }
   ],
   "source": [
    "getTuneStatusUrl = dl_rest_url + '/hypersearch'\n",
    "print ('getTuneStatusUrl: %s' %getTuneStatusUrl)\n",
    "r = req.get(getTuneStatusUrl, headers=commonHeaders, verify=False, auth=myauth)\n",
    "\n",
    "if not r.ok:\n",
    "    print('check hpo task status failed: code=%s, %s'%(r.status_code, r.content))\n",
    "else:\n",
    "    if len(r.json()) == 0:\n",
    "        print('There is no hpo task been created')\n",
    "    for item in r.json():\n",
    "        print('Hpo task: %s, State: %s'%(item['hpoName'], item['state']))\n",
    "        #print('Best:%s'%json.dumps(item.get('best'), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Model \n",
    "[back to top](#WMLA-HPO-Hyperband-Wells-EDT)\n",
    "\n",
    "We will update the model to enable:\n",
    " * Elastic Distributed Training\n",
    " * Hyper-parameter Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model file update to run Elastic Distributed Training\n",
    "\n",
    "Elastic Distributed Training (EDT) takes a model built on a standalone system,  and distributes model training across multiple GPUs and compute nodes in elastic fashion.   EDT starts job execution with minimal 1 GPU,  and GPUs can be added or removed dynamically while executing.\n",
    "\n",
    "We will use Elastic Distributed Training to distribute a single HPO task with multiple gpus.\n",
    "\n",
    "\n",
    "In this sample we will update the PyTorch RestNet18 model and enable Elastic Distributed Training: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "Model changes required from 3 perspective:\n",
    "* Import Elastic Distributed Training library and environment variable\n",
    "* Replace the data loading functions with ones that are compatible with EDT - the data loading function must return a tuple containing two items of type torch.utils.data.Dataset \n",
    "* Replace the training and testing loops with EDT’s train function\n",
    "\n",
    "\n",
    "##### Model update part 1.  Import Elastic Distributed Training library and environment variable\n",
    "\n",
    "<pre>\n",
    "path=os.path.join(os.getenv(\"FABRIC_HOME\"), \"libs\", \"fabric.zip\")\n",
    "print(path)\n",
    "sys.path.insert(0,path)\n",
    "from fabric_model import FabricModel\n",
    "from edtcallback import EDTLoggerCallback\n",
    "\n",
    "dataDir = environ.get(\"DATA_DIR\")\n",
    "if dataDir is not None:\n",
    "    print(\"dataDir is: %s\"%dataDir)\n",
    "else:\n",
    "    print(\"Warning: not found DATA_DIR from os env!\")\n",
    "</pre>\n",
    "\n",
    "    \n",
    "\n",
    "##### Model update part 2.  Replace the data loading functions with ones that are compatible with EDT - the data loading function must return a tuple containing two items of type torch.utils.data.Dataset \n",
    "\n",
    "* Download the Dataset from https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "\n",
    "<pre>\n",
    "def getDatasets():\n",
    "\n",
    "    data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "       transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    }\n",
    "\n",
    "    return (datasets.ImageFolder(os.path.join(dataDir, 'train'), data_transforms['train']),\n",
    "           datasets.ImageFolder(os.path.join(dataDir, 'val'), data_transforms['val']))\n",
    "</pre>         \n",
    "\n",
    "##### Model update part 3.  Replace the training and testing loops with EDT’s train function\n",
    "\n",
    " a. Instantiate Elastic Distributed Training instance\n",
    " <pre>\n",
    " edt_m = FabricModel(model, getDatasets, loss_function, optimizer, driver_logger=EDTLoggerCallback())\n",
    " </pre>\n",
    " b. Launch EDT job with specific parameters\n",
    " <pre>\n",
    " edt_m.train(epoch_number, effective_batch_size, max_number_GPUs_assigned_to_EDT_job)\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model file update to Run HPO\n",
    "\n",
    "Model changes required from 2 perspective:\n",
    "- Inject hyper-parameters for the sub-training during search\n",
    "- Retrieve sub-training result metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Model update part 1 - Inject hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The hyper-parameters will be supplied in a file called **config.json** with JSON format,located in the current working directory and can be read direcly as the following example snippet.\n",
    "\n",
    "<pre>\n",
    "hyper_params = json.loads(open(\"<b>config.json</b>\").read())\n",
    "learning_rate = float(hyper_params.get(\"<b>learning_rate</b>\", \"0.01\"))\n",
    "</pre>\n",
    "\n",
    "After this, you can use these hyper-parameters during the model trainings. The **hyper-parameter name** and **value** type is defined through the search space part in body of REST call when launching a new hpo task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Model update part 2 - Retrieve sub-training result metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At the end of your training run, your code will need to create a file called **val_dict_list.json** with test metrics generated during training. These metrics will be used by the search algorithm to propose new sets of hyper-parameters. Please note that **val_dict_list.json** should be created under the result directory which can be retrieved through the environment variable **RESULT_DIR**.\n",
    "\n",
    "<pre>\n",
    "with open('{}/val_dict_list.json'.format(os.environ['<b>RESULT_DIR</b>']), 'w') as f:\n",
    "    json.dump(test_metrics, f)\n",
    "</pre>\n",
    "\n",
    "We add this code in the function def on_train_end(self) defined in edtcallback.py  \n",
    "\n",
    "The content of **val_dict_list.json** will be some thing as below, **step** is some thing optional meaning the training iteration or epochs, one of **loss** and **accuracy** can be the name of target metric to optimize, at least one metric need to be included here. The specific name of metric used to optimize (minimize or maximize) is defined in the body of REST call when launching a new hpo task. \n",
    "\n",
    "```\n",
    "[\n",
    "{‘step’: 1, ‘loss’:0.2487, ‘accuracy’: 0.4523},\n",
    "{‘step’: 2, ‘loss’:0.1487, ‘accuracy’: 0.5523},\n",
    "{‘step’: 3, ‘loss’:0.1087, ‘accuracy’: 0.6523},\n",
    "…\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch HPO task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REST API: **POST /platform/rest/deeplearning/v1/hypersearch**\n",
    "- Description: Start a new HPO task\n",
    "- Content-type: Multi-Form\n",
    "- Multi-Form Data:\n",
    "  - files: Model files tar package, ending with `.modelDir.tar`\n",
    "  - form-filed: {‘data’: ‘String format of input parameters to start hpo task, let’s call it as **hpo_input** and show its specification later’}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Package model files for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package the updated model files into a tar file ending with `.modelDir.tar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "MODEL_DIR_SUFFIX = \".modelDir.tar\"\n",
    "tempFile = tempfile.mktemp(MODEL_DIR_SUFFIX)\n",
    "make_tarfile(tempFile, '/Users/Kelvin/Downloads/WellsFargo/CSSC_EDT/pytorch_edt_cssc')\n",
    "files = {'file': open(tempFile, 'rb')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct POST request data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hpo_input** will be a Python dict or json format as below, convert to string when calling REST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#'args': '--exec-start edtPyTorch --cs-datastore-meta type=fs, data_path=/dli_data_fs \\\n",
    "#                     --gpuPerWorker 1 --model-main pytorch_mnist.py --edt-options maxWorkers=8 \\\n",
    "#                     --model-dir pytorch_edt_hpo'\n",
    "\n",
    "data =  {\n",
    "        'modelSpec': # Define the model training related parameters\n",
    "        {\n",
    "            # Spark instance group which will be used to run the HPO sub-trainings. The Spark instance group selected\n",
    "            # here should match the sub-training args, for example, if the sub-training args try to run a EDT job,\n",
    "            # then we should put a Spark instance group with capability to run EDT job here.\n",
    "            'sigName': sigName,\n",
    "\n",
    "            # These are the arguments we'll pass to the execution engine; they follow the same conventions\n",
    "            # of the dlicmd.py command line launcher\n",
    "            #\n",
    "            # See:\n",
    "            #   https://www.ibm.com/support/knowledgecenter/en/SSFHA8_1.2.1/cm/dlicmd.html\n",
    "            # In this example, args after --model-dir are all the required parameter for the original model itself.\n",
    "            # maxWorker is the number of GPUs assigned to single HPO task\n",
    "            # \n",
    "            'args': '--model-dir pytorch_edt_cssc --exec-start edtPyTorch  \\\n",
    "                     --cs-datastore-meta type=fs,data_path=kelvin_data/hymenoptera_data --gpuPerWorker 1 \\\n",
    "                     --edt-options maxWorkers=8 --model-main pytorch_mnist_HPO_EDT.py \\\n",
    "                     --batchsize 16 --debug-level debug  \\\n",
    "                    '\n",
    "\n",
    "        },    \n",
    "        'algoDef': # Define the parameters for search algorithms\n",
    "        {\n",
    "            # Name of the search algorithm, one of Random, Bayesian, Tpe, Hyperband\n",
    "            'algorithm': 'Hyperband', \n",
    "            # Max running time of the hpo task in minutes, -1 means unlimited\n",
    "            'maxRunTime': -1,  \n",
    "            # Max number of training job to submitted for hpo task, -1 means unlimited’,\n",
    "            'maxJobNum': 5,            \n",
    "            # Max number of training job to run in parallel, default 1. It depends on both the\n",
    "            # avaiable resource and if the search algorithm support to run in parallel, current only Random\n",
    "            # fully supports to run in parallel, Hyperband and Tpe supports to to in parellel in some phase,\n",
    "            # Bayesian runs in sequence now.\n",
    "            'maxParalleJobNum': 1, \n",
    "            # Name of the target metric that we are trying to optimize when searching hyper-parameters.\n",
    "            # It is the same metric name that the model update part 2 trying to dump.\n",
    "            'objectiveMetric' : 'loss',\n",
    "            # Strategy as how to optimize the hyper-parameters, minimize means to find better hyper-parameters to\n",
    "            # make the above objectiveMetric as small as possible, maximize means the opposite.\n",
    "            'objective' : 'minimize',\n",
    "            # eta value to control the hyper-band search process\n",
    "            'hyperbandEta': 3.0,\n",
    "            #Additional parameters for the specified search algorithm and hyper-band get following too.\n",
    "            'algoParams' : \n",
    "                [\n",
    "                    {\n",
    "                        # Name of the the maximum amount of resource that can be allocated to a single configuration\n",
    "                        'name':'ResourceName', \n",
    "                        'value': 'epochs'\n",
    "                    },\n",
    "                    {\n",
    "                        # Value of the the maximum amount of resource that can be allocated to a single configuration\n",
    "                        'name':'ResourceValue',\n",
    "                        'value':'1'\n",
    "                    }\n",
    "                    # This resource parameter will change during hyperband searching phase and its value will be put\n",
    "                    # into the config.json for each sub-training too, here with the 'epoch' as key and value in the\n",
    "                    # range 1-10.\n",
    "                ]\n",
    "        },\n",
    "    \n",
    "        # Define the hyper-paremeters to search and the corresponding search space.\n",
    "        'hyperParams':\n",
    "        [\n",
    "             {\n",
    "                 # Hyperparameter name, which will be the hyper-parameter key in config.json\n",
    "                 'name': 'learning_rate',\n",
    "                 # One of Range, Discrete\n",
    "                 'type': 'Range',\n",
    "                 # one of int, double, str\n",
    "                 'dataType': 'DOUBLE',\n",
    "                 # lower bound and upper bound when type=range and dataType=double\n",
    "                 'minDbVal': 0.001,\n",
    "                 'maxDbVal': 0.1,\n",
    "                 # lower bound and upper bound when type=range and dataType=int\n",
    "                 'minIntVal': 0,\n",
    "                 'maxIntVal': 0,\n",
    "                 # Discrete value list when type=discrete\n",
    "                 'discreteDbVal': [],\n",
    "                 'discreteIntVal': [],\n",
    "                 'discreateStrVal': [],\n",
    "                 #step size to split the Range space. ONLY valid when type is Range\n",
    "                 'step': '0.002',\n",
    "             }\n",
    "         ]\n",
    "    }\n",
    "mydata={'data':json.dumps(data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit the Post request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit hpo task through the Post call and a hpo name/id as string format will get back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Application: Admin-hpo-131842408409190\n"
     ]
    }
   ],
   "source": [
    "startTuneUrl=dl_rest_url + '/hypersearch'\n",
    "r = req.post(startTuneUrl, headers=commonHeaders, data=mydata, files=files, verify=False, auth=myauth)\n",
    "if r.ok:\n",
    "    hpoName = r.json()\n",
    "    json_out_1 = r.json()\n",
    "    print ('HPO Application: %s' %json_out_1)\n",
    "\n",
    "else: \n",
    "    print('\\nModel submission failed with code={}, {}'. format(r.status_code, r.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query GPU allocation per HPO Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Sig ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getsigidurl = sc_rest_url + '/instances?name={}'.format(sigName) + '&fields=id&compareop_name=='\n",
    "#getsigidurl = sc_rest_url + '/instances?name={}'.format(sigName)\n",
    "#print (getsigidurl)\n",
    "res = req.get(getsigidurl, headers=commonHeaders, verify=False, auth=('Admin', 'Admin'))\n",
    "if res.ok:\n",
    "    json_out=res.json()\n",
    "    sig_id = res.json()[0]['id']['uuid']\n",
    "    print ('SIG id: %s' %sig_id)\n",
    "    \n",
    "    #print (pd.read_json(json.dumps(json_out)))\n",
    "   \n",
    "else: \n",
    "    print('\\SIG ID retrival failed with code={}, {}'. format(r.status_code, r.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status of all RUNNING jobs in SIG (rerun cell to refresh)\n",
    "\n",
    "monitor = []\n",
    "monitor_output = []\n",
    "\n",
    "r = requests.get(sc_rest_url+'/instances/'+sig_id+'/applications?state=RUNNING', \n",
    "                auth=('Admin', 'Admin'), headers=commonHeaders, verify=False).json()\n",
    "\n",
    "if (len(r) == 0):\n",
    "    print ('No jobs running')\n",
    "    \n",
    "else:\n",
    "    # Filter out the relevant information\n",
    "  \n",
    "    monitor.append([(\n",
    "        job['driver']['id'],\n",
    "        job['applicationname'],\n",
    "        job['applicationid'],\n",
    "        job['driver']['state'],\n",
    "        job['apprunduration'],\n",
    "        job['gpuslots'],\n",
    "        #job['gpumemused']['total'],\n",
    "        #job['gpudevutil']['total'],\n",
    "    ) for job in r])\n",
    "\n",
    "    monitor_output = pd.DataFrame([item for monitor in monitor for item in monitor])\n",
    "    monitor_output.columns = [\n",
    "        'Driver ID',\n",
    "        'Application Name', \n",
    "        'Appliation ID',\n",
    "        'State', \n",
    "        'Run duration (mins)',\n",
    "        'GPU slots',\n",
    "        #'Total GPU memory used',\n",
    "        #'Total GPU utilsation (%) ',\n",
    "    ]\n",
    "\n",
    "monitor_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check HPO task status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REST API: **GET /platform/rest/deeplearning/v1/hypersearch/{hponame}**\n",
    "- Description: Retrieve the hpo task details with the specified hpo task name/id in URL.\n",
    "- OUTPUT: A particular hpo task with details of the specified hponame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#hpoName = 'Admin-hpo-121935856803278'\n",
    "\n",
    "getHpoUrl = dl_rest_url +'/hypersearch/'+ hpoName\n",
    "\n",
    "res = req.get(getHpoUrl, headers=commonHeaders, verify=False, auth=('Admin', 'Admin'))\n",
    "if not res.ok:\n",
    "    print('get hpo task failed: code=%s, %s'%(res.status_code, res.content))\n",
    "else:\n",
    "    json_out=res.json()\n",
    "\n",
    "    while json_out['state'] in ['RUNNING']:\n",
    "        print('Hpo task %s state %s progress %s%%'%(hpoName, json_out['state'], json_out['progress']))\n",
    "        time.sleep(60)\n",
    "        res = req.get(getHpoUrl, headers=commonHeaders, verify=False, auth=myauth)\n",
    "        json_out=res.json()\n",
    "\n",
    "    print('Hpo task %s completes with state %s'%(hpoName, json_out['state']))\n",
    "    print(json.dumps(json_out, indent=4, sort_keys=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full Response of a hpo task status.**\n",
    "\n",
    "```\n",
    "{\n",
    "\"hpoName\": \"string, name/id of the hpo task\",\n",
    "\"state\": \"string, hpo task state, SUBMITTED, RUNNING, FAILED, FINISHED, STOPPED\",\n",
    "\"running\": \"int, number of training that is under-going right now\",\n",
    "\"complete\": \"int, number of training that has completes, including both succeeded and failed trainings\",\n",
    "\"failed\": \"int, number of training that failed\",\n",
    "\"progress\": \"string, progress of a percentage value\",\n",
    "\"createtime\": \"string, time this task was created\",\n",
    "\"creator\": \"string, user name who created this hpo task\",\n",
    "\"duration\": \"string, how long the task been run with format hh-mm-ss\",\n",
    "\"experiments\": [\n",
    "                {\n",
    "                \"id\": \"int, counter id of the experiment training, start from 0\",\n",
    "                \"metricVal\": \"double, best metric value of the experiment training\",\n",
    "                \"state\": \"string, state of the BYOF training task\"\n",
    "                \"appId\": \"string, BYOF training task id\",\n",
    "                \"driverId\": \"string, for internal usage, the real job id for training, currently it is the spark driver id\",\n",
    "                \"hyperParams\": [\n",
    "                                {\n",
    "                                \"name\": \"string, name of the hyperparameter\",\n",
    "                                \"dataType\": \"string, data type of the hyperparameter\",\n",
    "                                \"fixedVal\": \"string, The hyperparamter value been used in this experiment training with the same datatype as input dataType\"\n",
    "                                }\n",
    "                                ],\n",
    "                }\n",
    "                ],\n",
    "\"best\": {\"one of the experiments with the best metric value, smallest or biggest one based on the original objective minimize or maximize\"}\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage HPO tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Check HPO tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Same REST api as the early health check one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#getTuneStatusUrl = 'https://{}:9243/platform/rest/deeplearning/v1/hypersearch'.format(hostname)\n",
    "res = req.get(startTuneUrl, headers=commonHeaders, verify=False, auth=('Admin', 'Admin'))\n",
    "if not res.ok:\n",
    "    print('check tune job status failed: code=%s, %s'%(res.status_code, res.content))\n",
    "else:\n",
    "    #print(json.dumps(r.json(), sort_keys=True, indent=4))\n",
    "    if len(res.json()) == 0:\n",
    "        print('There is no hpo task been created')\n",
    "    for item in res.json():\n",
    "        #print(item['hpoName'])\n",
    "        print('Hpo task: %s, State: %s'%(item['hpoName'], item['state']))\n",
    "        #print('Hpo tasks detail:%s'%json.dumps(item, sort_keys=True, indent=4))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Stop Hpo task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "REST API: **PUT /platform/rest/deeplearning/v1/hypersearch/{hponame}**\n",
    "- Description: Stop the specified hpo job with name/di {hponame}\n",
    "- OUTPUT: success http return code\n",
    "\n",
    "Since the hpo job has already finished, so nothing to stop here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopHpoUrl = 'https://{}:9243/platform/rest/deeplearning/v1/hypersearch/{}'.format(hostname, hpoName)\n",
    "r=req.put(stopHpoUrl,headers=commonHeaders, verify=False, auth=('Admin','Admin'))\n",
    "if not r.ok:\n",
    "    print('stop hpo task failed: code=%s, %s'%(r.status_code, r.content))\n",
    "else:\n",
    "    print('stop hpo task succeeds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Hpo tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REST API: **DELETE /platform/rest/deeplearning/v1/hypersearch/{hponame}**\n",
    "- Description: Delete the specified hpo task with name/id {hponame}\n",
    "- OUTPUT: success http return code\n",
    "\n",
    "REST API: **DELETE /platform/rest/deeplearning/v1/hypersearch**\n",
    "- Description: Delete all the hpo task that the login user can access. But please be aware, the BYOF training been created by this hpo task will not be deleted.\n",
    "- OUTPUT: success http return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleteHpoUrl = 'https://{}:9243/platform/rest/deeplearning/v1/hypersearch'.format(hostname)\n",
    "r=req.delete(deleteHpoUrl,headers=commonHeaders, verify=False, auth=('Admin','Admin'))\n",
    "if not r.ok:\n",
    "    print('delete hpo task failed: code=%s, %s'%(r.status_code, r.content))\n",
    "else:\n",
    "    print('delete hpo task succeeds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
